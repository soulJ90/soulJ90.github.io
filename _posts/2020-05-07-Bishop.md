2020-05-07
# 1 Introduction

###  History of pattern recognition 

 -   In the 16th century  **Astronomical observation**  :  discovering the empirical laws of planetary motion. 
 -  early twentieth century, **Quantum physics**  :  discovering of regularities in  atomic spectra.
- In **Machine learning** :  automatic discovering of regularities in data through the use of computer algorithms and taking actions such as classifying the data into different categories.

### Types of pattern recognition problem

 1. Supervised  learning

    +  The training data comprises examples of the input vectors along with their corresponding target vector.
    + ex) Classification, Regression 

 2. Unsupervised learning
    + The training data consists of a set of input vectors x without any corresponding target value.
    + ex) Clustering, Density estimation, Visualization using dimension reduction

 3. Reinforcement learning (SuttonandBarto,1998)

     + The problem of ﬁnding suitable actions to take in a given situation in order to maximize a reward.
     + In contrast to supervised learning, the learning algorithm is not given examples of optimal outputs, but must instead discover them by a process of trial and error. 
     + Typically there is a sequence of states and actions in which the learning algorithm is interacting with its environment.
     + Trade off between *exploration* and *exploitation*
     
    > *exploration* : the system tries out new kinds of actions to see how effective they are.
    > *exploitation* : the system makes use of actions that are known to yield a high reward.
    


### Basic part in the application of machine learning

* Probability theory  
* Decision theory
* Informatio theory 

 
 
# 1.1 Example : Polynomial Curve Fitting

##  Simple regression problem 
![figure1.2](https://www.dropbox.com/s/cto310s5zi6t436/Figure12.PNG?dl=1)
Figure1.2  plot of a trainign data set of $N$=10 and the underyling funtion $sin(2\pi x )$.


* Data generation : the data is generated from the function $sin(2\pi x )$ with random noise included in the target values. (Figure 1.2)

* training set : Suppose that training set comprising $N$ observations of $x$ as follows, $\mathbf{x}$  := $(x_1, ..., x_N)$ and corresponding target values, denoted $\mathbf{t}:=(t_1, ..., t_N)^T$. 

* Goal : to make predictions of the target value for some new input values. $\Leftarrow$ discovering the underlying funtion $sin(2\pi x)$

* Challenge :  this regression problem is intrinsically a difficult problem as we have to  **generalize from a finite dataset**  and **the observed data are corrupted with noise**. 

* Relative theory to solve the problem : Probability theory (expresisng  such uncertainty in a precise and quantitative manner) and decision theory. 

## Polynomial function

In particular, we shall fit the data using a polynomial funtion of the form

 $$y(x,\mathbf{w})=w_0+w_1x+w_2x^2+\cdots+w_Mx^M=\Sigma^M_{j=0}w_jx^j$$, 
 
  * where $M$ is the *order* of the polynomial, $x^j$ is denotes $x$ raised to the power of $j$. 
  * $\mathbf{w}$ is vector of the coefficients $w_0,\cdots,w_M$ 


### A) Estimation of the coefficients

The values of the coefﬁcients will be determined by ﬁtting the polynomial to the training data. This can be done by minimizing an error function that measures the misﬁt between the function $y(x,w)$, for any given value of $w$, and the training set data points.

One simple choice of error function is *sum of the squares of the error* 

 $$E(w)=\frac12\Sigma^N_{n=1}[y(x_n,\mathbf{w})-t_n]^2$$

* Error function is a quardaratic function of the coeficients $w$. 
* Derivatives with respect to the coefficients will be linear in $w$.
* So, the minimization of the error function has a unique solution.

![figure1.3](https://www.dropbox.com/s/8uiaq5izvj8qtyu/Figure13.PNG?dl=1)
<center>Figure1.3  The implementation of the error function  </center>


###   B) Choosing the order $M$ of the polynomial

We show four exaples of the result of ﬁtting polynomials having orders $M =0 ,1,3$, and $9$ to the data set shown in Figure 1.2. 

* $M =0$ and $M =1$ : poor ﬁts to the data and the function $sin(2πx)$. 
* $M =3$: the best ﬁt to the function $sin(2πx)$ of the examples. 
* $M =9$ : an excellent ﬁt to the training data ($\Leftrightarrow E(w) =0$).  However, the ﬁtted curve oscillates wildly and gives a very poor representation of the function $sin(2πx)$ . This  behaviour is known as **over-ﬁtting**. Bad generalization!! 


![figure1.4](https://www.dropbox.com/s/ujzhoyf1cv5lq54/Figure14.PNG?dl=1 )


Another choice of error function is **root-mean-square(RMS) error** defined by 

$$E_{RMS}=\sqrt{2E(w^*)/N}$$.
   
   * in which the division by $N$  allows us to compare different sizes of datasets.
   * square root ensures that $E_{RMS}$  is measured on the same scale (and in the same units) as the target variable. 


![figure1.5](https://www.dropbox.com/s/01htbf7lze1u9q5/Figure15.PNG?dl=1 )
<center>Figure1.5   plot of root-mean-square-error for various values of M. </center>

* $M=0$ and $M=1$ : poor ﬁts to the test, training datasets and the function $sin(2πx)$. 
* $M=3$: the best ﬁt to the function $sin(2πx)$ of the examples. 
* $M=9$ : the training set error goes to zero (df=10, $N$=10), but  test set error has become very large (over-fitting) 


## Even though the model with M=9 includes M=3, why is M=9 performing worse? 

This may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases. The $M =9$ polynomial is therefore capable of generating results at least as good as the $M =3$ polynomial.  so we might expect that results should improve monotonically as we increase $M$.

Here's a hint for above question. 
					       ![Table1.1](https://www.dropbox.com/s/qav7g8szdrdltml/Table11.PNG?dl=1)
 <center>Table 1.1  Table of the coefficients w* for the various order M. </center>

* In particular for the $M=9$ , the magnitude of coefficient $w*$ is very large.  
* This is because the degree of freedom of model is similar to $N$ .
* The coefficient have become ﬁnely tuned to the each data point.
*  Intuitively, what is happening is that the more ﬂexible polynomials with larger values of $M$ are becoming increasingly tuned to the random noise on the target values. 

## An alternative to over-fitting

 1. Incresing the size of the dataset reduces the over-fitting problem. (See details in Figure 1.6)

 2. By adopting a *Bayesian approach*, the over-fitting can be avoided.
	(Indeed, in a Bayesian model the effective number of parameters adapts automatically to the size of the data set.)
  
  3. *Regularization* : adding a penalty term to the error function in order to discourage the coefﬁcients from reaching large values. The error function of  *rigde regression*, which is a representative method of *Regularization*, is as follows.

$$\tilde{E}(w)=\frac{1}{2} \Sigma^N_{n=1}[y(x_n,\mathbf{w})-t_n]^2+\frac{\lambda}{2} ||\mathbf{w}||^2$$


* $\lambda$ governs the relative importance of the regularization term compared with the sum-of-squares error term.
* $\parallel \mathbf{w} \parallel^2=\mathbf{w}^T \mathbf{w}=w^2_0+w^2_1+\cdots+w^2_M$

* $\tilde{E}(w)$ can be minimized exactly in closed form. 
* this is known as *shrinkage method* (reduce the values of the coefficients)
* In the context of neural networks, this approach is known as weight decay. 

![figure1.6](https://www.dropbox.com/s/7yy3rm3kggjryt7/Figure16.PNG?dl=1 )
 <center> Figure 1.6  Plot of solutions using M=9 polynomial for N=15(left) and N=100 (right). </center>



![figure1.7](https://www.dropbox.com/s/y6yjql8bhuxwayt/Figure17.PNG?dl=1 )
 <center> Figure 1.7  Plot of  M=9 polynomial fitted to the dataset in figure 1.2 using regularized error function for two values of lambda. </center>.
  
 ![figure1.8](https://www.dropbox.com/s/di3kwvupjki6gr9/Figure18.PNG?dl=1)

 Figure 1.8  Plot of root-mean-square error versus ln(lamda) for M=9.
 

# 1.2 Probability Theory

A key concept in the ﬁeld of pattern recognition is that of uncertainty. It arises both through noise on measurements, as well as through the ﬁnite size of data sets. Probability theory provides a consistent framework for the quantiﬁcation and manipulation of uncertainty and forms one of the central foundations for pattern recognition


##  The Rules of Probability

### 1) **Sum rule**

$$p(x) = \Sigma_Y p(X,Y)$$

### 2) **Product rule**

$$p(X,Y)=p(Y|X)p(x)$$

where $p(X,Y)$ is a *joint probability*, $p(Y|X)$ is a *conditional probability* and $p(x)$ is a *marginal probability*.

### 3) **Bayes' theorem**

$$p(Y|X)=\frac{p(X|Y)p(Y)} {p(X)}$$

Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator $p(X)=\Sigma_{Y}p(X|Y)p(Y)$.


##  Example of Joint, conditional and marginal Probability


Figure1.11, we show a simple example involving a joint distribution over two variables to illustrate the concept of marginal and conditional distributions. Here a ﬁnite sample of $N=60$ data points has been drawn from the joint distribution and is shown in the top left. In the top right is a histogram of the fractions of data points having each of the two values of $Y$. From the deﬁnition of probability, these fractions would equal the corresponding probabilities $p(Y)$ in the limit $N→\infty$. The remaining two plots in Figure 1.11 show the corresponding histogram estimates of $p(X)$ and $p({X|Y}=1)$. 


![figure1.11](https://www.dropbox.com/s/6sw2cgonwnlfwjj/Figure111.PNG?dl=1)
 <center> Figure 1.11  An illustration of a distribution over two variables, X, which takes 9 possible values, and Y , which takes two possible values.  </center>
 

## 1.2.1 Probability densities

### 1) **Probability density**

If the probability of a real-valued variable x falling in the interval $(x,x + δx)$ is given by $p(x)δx$ for $δx → 0$, then $p(x)$ is called the probability density over x.
	 
$$p(x\in (a,b))=\int_{a}^{b} p(x)dx.$$

* The probability density p(x) must satisfy the two conditions 

	 - $p(x) \geq 0$
	 - $\int_{-∞}^{∞} p(x)dx =1.$

### 2) **Jacobian factor**

For instance, if we consider a change of variables $x = g(y)$, then a function $f(x)$ become $\tilde{f(y)} =f(g(y))$. Now consider a probability density $p_x(x)$ that corresponds to a density $p_y(y)$ with respect to the new variable y, where the sufﬁces denote the fact that $p_x(x)$ and $p_y(y)$ are different densities.  Observations falling in the range $(x,x + δx)$ will, for small values of $δx$, be transformed into the range $(y,y + δy)$ where $p_x(x)δx \simeq p_y(y)δy$, and hence 

$$p_y(y)=p_x(x)| {\frac{dx}{dy}}|=p_x(g(y))|g'(y)|.$$

### 3)  **cumulative distribution function**

The probability that $x$ lies in the interval $(−∞,z)$ is given by the *cumulative distribution function* deﬁned by deﬁned by 

$$P(z) =\int_{-∞}^{z} p(x)dx.$$
	
which satisﬁes $P'(x)=p(x)$, as shown in Figure 1.12. 
	  
![figure1.12](https://www.dropbox.com/s/iyqo0xen3ev8b1s/Figure112.PNG?dl=1)
 <center> Figure 1.12  The concept of probability and The probability density can be expressed as the derivative of a cumulative distribution function P(x).
  </center>


### 4) **Sum and product rules and Bayes’ theorem**

The sum and product rules of probability, as well as Bayes’ theorem, apply equally to the case of probability densities.

$$p(x) =\int p(x,y)dy$$ $$p(x,y)=p(y|x)p(x).$$


## 1.2.2 Expectations and covariances

### 1) **Expectations**

 The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of $f(x)$ and will be denoted by $E[f]$. 

* For a discrete distribution,

$$E[f]=\Sigma_x p(x)f(x).$$

* For a continuous distribution,

 $$E[f]=\int p(x)f(x)dx.$$ 
 
* In either case, if we are given a ﬁnite number N from the probability distribution or probability density, then the expectation can be approximated as 

$$E[f]\approx \frac{1}{N} \Sigma^N_{n=1} f(x_n)$$ 


### 2) **Expectations of functions of several variables**

Sometimes we will be considering expectations of functions of several variables, in which case we can use a subscript to indicate which variable is being averaged over, so that for instance $E_x[f(x,y)]$ 

* Conditional expectation

$$E_x{[f|y]} =\Sigma_x p(x|y)f(x)$$

### 3)  **Variance**

$$Var[f]=E(f(x)−E[f(x)])^2$$ $$Var[f]=E[f(x)^2]−E[f(x)]^2$$

### 4)  **Covariance**

* For two random variables x and y, the covariance is deﬁned by 

$$Cov[x,y]=E_{x,y}[(x−E(x))(y−E[y])]$$ $$Cov[x,y]= E_{x,y}[xy]−E[x]E[y].$$

 If x and y are independent, then their covariance vanishes.

* In the case of two vectors of random variables x and y, the covariance is a matrix

$$Cov[x,y]= E_{x,y}[xy^T]−E[x] E[y ^T].$$ 


